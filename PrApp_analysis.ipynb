{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrApp analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tableone import TableOne\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from kneefinder import KneeFinder\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.patches as mpatches\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read preprocessed PrApp Data \n",
    "PrEP = pd.read_csv('data/PrEP_filtered.csv') \n",
    "no_PrEP = pd.read_csv('data/no_PrEP_filtered.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for PrEP-indication\n",
    "### Non-PrEP users with PrEP-indication\n",
    "\n",
    "**Filter criteria for PrEP-indication (at least one criterion met)**\n",
    "- sexualized drug use in the last 6 months\n",
    "- STI in the last 12 months (syphilis, gonorrhoea, chlamydia, hepatitis C)\n",
    "- ≥ 2 sex partners in the last 6 months AND low condom use (0-50%)\n",
    "\n",
    "Steps\n",
    "- Total number of non-PrEP users (preprocessed): 1,420 \n",
    "- Non-PrEP users with PrEP indication: 431 \n",
    "- Non-PrEP users without PrEP indication: 989\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of non-PrEP users (preprocessed): {no_PrEP.shape[0]}')\n",
    "\n",
    "no_PrEP_indication = no_PrEP.loc[(no_PrEP['sexualized_drug_use'] == 'Ja') |                                         # drugs during sex       \n",
    "                                 ((no_PrEP['test_syp_freq'] == '1') | (no_PrEP['test_syp_freq']  == '2+')) |        # STI: Syphilis\n",
    "                                 ((no_PrEP['test_go_freq'] == '1') | (no_PrEP['test_go_freq']  == '2+')) |          # STI: Gonorrhö \n",
    "                                 ((no_PrEP['test_chla_freq'] == '1') | (no_PrEP['test_chla_freq']  == '2+')) |      # STI: Chlamydien \n",
    "                                 (no_PrEP['test_hcv_freq'] == 1.0) |                                                # STI: Hepatitis \n",
    "                                 (((no_PrEP['sex_partner'] == '2-3') |                                              # 2 or more sex partners in the last 6 months \n",
    "                                   (no_PrEP['sex_partner'] == '4-5') | (no_PrEP['sex_partner'] == '6-10') |\n",
    "                                   (no_PrEP['sex_partner'] == '11-20') | (no_PrEP['sex_partner'] == '>20')) \n",
    "                                  & ((no_PrEP['condom'] == '0%') |                                                  # AND low condom use (0-50%)\n",
    "                                     (no_PrEP['condom'] == '25%') | (no_PrEP['condom'] == '50%')))]  \n",
    "print(f'Number of non-PrEP users with PrEP-indication: {no_PrEP_indication.shape[0]}')\n",
    "\n",
    "no_PrEP_remaining = no_PrEP.loc[~no_PrEP.index.isin(no_PrEP_indication.index)] \n",
    "print(f'Number of non-PrEP users without PrEP-indication: {no_PrEP_remaining.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PrEP users with PrEP-indication\n",
    "\n",
    "**Filter criteria for PrEP-indication (at least one criterion met)**\n",
    "- drugs during sex in the last 6 months\n",
    "- STI in the last 12 months (syphilis, gonorrhoea, chlamydia, hepatitis C)\n",
    "- ≥ 2 sex partners in the last 6 months \n",
    "\n",
    "Steps\n",
    "- Total number of PrEP users (preprocessed): 1,061 \n",
    "- PrEP users with PrEP-indication: 1,027 \n",
    "- PrEP users without PrEP-indication: 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of PrEP users (preprocessed): {PrEP.shape[0]}')\n",
    "\n",
    "PrEP_indication = PrEP.loc[(PrEP['sexualized_drug_use'] == 'Ja') |                                                  # drugs during sex \n",
    "                           ((PrEP['test_syp_freq'] == '1') | (PrEP['test_syp_freq']  == '2+')) |                    # STI: Syphilis\n",
    "                           ((PrEP['test_go_freq'] == '1') | (PrEP['test_go_freq']  == '2+')) |                      # STI: Gonorrhö \n",
    "                           ((PrEP['test_chla_freq'] == '1') | (PrEP['test_chla_freq']  == '2+')) |                  # STI: Chlamydien \n",
    "                           (PrEP['test_hcv_freq'] == 1.0) |                                                         # STI: Hepatitis \n",
    "                           ((PrEP['sex_partner'] == '2-3') |                                                        # 2 or more sex partners in the last 6 months\n",
    "                            (PrEP['sex_partner'] == '4-5') | (PrEP['sex_partner'] == '6-10') |\n",
    "                            (PrEP['sex_partner'] == '11-20') | (PrEP['sex_partner'] == '>20'))]    \n",
    "print(f'Number of PrEP users with PrEP-indication: {PrEP_indication.shape[0]}')\n",
    "\n",
    "PrEP_remaining = PrEP.loc[~PrEP.index.isin(PrEP_indication.index)] \n",
    "print(f'Number of PrEP users without PrEP-indication: {PrEP_remaining.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratification by survey wave \n",
    "\n",
    "- Non-PrEP users with PrEP-indication: 431 (wave 3: 349 and wave 4: 82)\n",
    "- Non-PrEP users without PrEP-indication: 989 (wave 3: 772 and wave 4: 217)\n",
    "\n",
    "- PrEP users with PrEP-indication: 1,027 (wave 3: 759 and wave 4: 268)\n",
    "- PrEP users without PrEP-indication: 34 (wave 3: 22 and wave 4: 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Non-PrEP users with a PrEP-indication, in wave 3:\", len(no_PrEP_indication[no_PrEP_indication['welle'] == 3.0]))\n",
    "print(\"Non-PrEP users with a PrEP-indication, in wave 4:\", len(no_PrEP_indication[no_PrEP_indication['welle'] == 4.0]))\n",
    "\n",
    "print(\"\\nNon-PrEP users without a PrEP-indication, in wave 3:\", len(no_PrEP_remaining[no_PrEP_remaining['welle'] == 3.0]))\n",
    "print(\"Non-PrEP users without a PrEP-indication, in wave 4:\", len(no_PrEP_remaining[no_PrEP_remaining['welle'] == 4.0]))\n",
    "\n",
    "print(\"\\nPrEP users with a PrEP-indication, in wave 3:\", len(PrEP_indication[PrEP_indication['welle'] == 3.0]))\n",
    "print(\"PrEP users with a PrEP-indication, in wave 4:\", len(PrEP_indication[PrEP_indication['welle'] == 4.0]))\n",
    "\n",
    "print(\"\\nPrEP users without a PrEP-indication, in wave 3:\", len(PrEP_remaining[PrEP_remaining['welle'] == 3.0]))\n",
    "print(\"PrEP users without a PrEP-indication, in wave 4:\", len(PrEP_remaining[PrEP_remaining['welle'] == 4.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgroups in non-PrEP users with PrEP-indication \n",
    "**Two subgroups in non-PrEP users with PrEP-indication (n = 431)**\n",
    "- PrEP-indication induced by sexualized drug use: 130\n",
    "- PrEP-indication induced by sexual behaviour (two or more sexual partner AND low condom use) and/or STI: 364\n",
    "\n",
    "no exclusive groups: 63 participants are in both groups (have a PrEP-indication due to sexual behaviour/STI and sexualized drug use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-PrEP users with a PrEP-indication due to their sexualized drug use in the last 6 months (n = 130)\n",
    "no_PrEP_drug = no_PrEP_indication.loc[(no_PrEP_indication['sexualized_drug_use'] == 'Ja')] \n",
    "print(f'Number of PrEP users with PrEP-indication due to their sexualized drug use in the last 6 months: {no_PrEP_drug.shape[0]}')\n",
    "\n",
    "# Non-PrEP users with a PrEP-indication due to a STI in the last 12 months (n = 143)\n",
    "no_PrEP_STI = no_PrEP_indication.loc[((no_PrEP['test_syp_freq'] == '1') | (no_PrEP['test_syp_freq']  == '2+')) |        # STI: Syphilis\n",
    "                                      ((no_PrEP['test_go_freq'] == '1') | (no_PrEP['test_go_freq']  == '2+')) |         # STI: Gonorrhö \n",
    "                                      ((no_PrEP['test_chla_freq'] == '1') | (no_PrEP['test_chla_freq']  == '2+')) |     # STI: Chlamydien \n",
    "                                      (no_PrEP['test_hcv_freq'] == 1.0) ]                                               # STI: Hepatitis C\n",
    "print(f'Number of non-PrEP users with PrEP-indication due to a STI in the last 12 months: {no_PrEP_STI.shape[0]}')\n",
    "\n",
    "# Non-PrEP users with a PrEP-indication due to their sexual behaviour (n = 266)\n",
    "# sexual behaviour: two or more sexual partners in the last 6 months and low condom use (0-50%)\n",
    "no_PrEP_sex = no_PrEP_indication.loc[(((no_PrEP_indication['sex_partner'] == '2-3') | (no_PrEP_indication['sex_partner'] == '4-5') | \n",
    "                                       (no_PrEP_indication['sex_partner'] == '6-10') | (no_PrEP_indication['sex_partner'] == '11-20') | (no_PrEP_indication['sex_partner'] == '>20')) &\n",
    "                                       ((no_PrEP_indication['condom'] == '0%') | (no_PrEP_indication['condom'] == '25%') | (no_PrEP_indication['condom'] == '50%')))]  \n",
    "print(f'Number of non-PrEP users with PrEP-indication due to their sexual behaviour in the last 6 months: {no_PrEP_sex.shape[0]}')\n",
    "\n",
    "# Non-PrEP users with a PrEP-indication due to their sexual behaviour and/or STI history (n = 364)\n",
    "no_PrEP_STI_and_sex = no_PrEP_indication.loc[(((no_PrEP['test_syp_freq'] == '1') | (no_PrEP['test_syp_freq']  == '2+')) |        \n",
    "                                              ((no_PrEP['test_go_freq'] == '1') | (no_PrEP['test_go_freq']  == '2+')) |           \n",
    "                                              ((no_PrEP['test_chla_freq'] == '1') |  (no_PrEP['test_chla_freq']  == '2+')) | (no_PrEP['test_hcv_freq'] == 1.0)) |      \n",
    "                                             (((no_PrEP_indication['sex_partner'] == '2-3') | (no_PrEP_indication['sex_partner'] == '4-5') | \n",
    "                                               (no_PrEP_indication['sex_partner'] == '6-10') |(no_PrEP_indication['sex_partner'] == '11-20') | (no_PrEP_indication['sex_partner'] == '>20')) &\n",
    "                                               ((no_PrEP_indication['condom'] == '0%') | (no_PrEP_indication['condom'] == '25%') | (no_PrEP_indication['condom'] == '50%')))]  \n",
    "print(f'Number of non-PrEP users with PrEP-indication due to their sexual behaviour and/or STI history: {no_PrEP_STI_and_sex.shape[0]}')\n",
    "\n",
    "\n",
    "# Non-PrEP users with a PrEP-indication due to their sexual behaviour and/or STI history AND sexualized drug use (n = 63)\n",
    "common_indices = no_PrEP_STI_and_sex.index.intersection(no_PrEP_drug.index)\n",
    "common_cases = no_PrEP_STI_and_sex.loc[common_indices] # 63 common participants\n",
    "print(f'Number of non-PrEP users with PrEP-indication due to their sexual behaviour and/or STI history AND sexualized drug use: {common_cases.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exclusive subgroups in non-PrEP users with PrEP-indication**\n",
    "- PrEP-indication exclusively induced by sexualized drug use: 67\n",
    "- PrEP-indication exclusively induced by sexual behaviour and/or STI: 301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_PrEP_drug.loc[:, 'INDICATION_SUBGROUP'] = 'sexualized drug-use'\n",
    "no_PrEP_STI_and_sex.loc[:, 'INDICATION_SUBGROUP'] = 'sexual behaviour or STI'\n",
    "\n",
    "# add 'both' as subgroup (both sexualized drug use and STI history and/or sexual behaviour)\n",
    "no_PrEP_drug_both = no_PrEP_drug.copy()\n",
    "no_PrEP_drug_both.loc[no_PrEP_drug_both.index.isin(common_cases.index), 'INDICATION_SUBGROUP'] = 'both' # 63 common participants with 'both'\n",
    "no_PrEP_STI_and_sex_both = no_PrEP_STI_and_sex.copy()\n",
    "no_PrEP_STI_and_sex_both.loc[no_PrEP_STI_and_sex_both.index.isin(common_cases.index), 'INDICATION_SUBGROUP'] = 'both' # 63 common participants with 'both'\n",
    "\n",
    "# exclusive subgroups (without common cases)\n",
    "no_PrEP_drug_exclusive = no_PrEP_drug.copy()\n",
    "no_PrEP_drug_exclusive = no_PrEP_drug_exclusive[~no_PrEP_drug_exclusive.index.isin(common_cases.index)]\n",
    "no_PrEP_STI_and_sex_exclusive = no_PrEP_STI_and_sex.copy()\n",
    "no_PrEP_STI_and_sex_exclusive = no_PrEP_STI_and_sex_exclusive[~no_PrEP_STI_and_sex_exclusive.index.isin(common_cases.index)]\n",
    "\n",
    "print(f'Non-PrEP users with a PrEP-indication exclusively due to sexualized drug use: {len(no_PrEP_drug_exclusive)}')\n",
    "print(f'Non-PrEP users with a PrEP-indication exclusively due to sexual behaviour and/or STI history: {len(no_PrEP_STI_and_sex_exclusive)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate PrEP user and non-PrEP users with a PrEP-indication for descriptive summary\n",
    "concatenated_all_indication = pd.concat([PrEP_indication, no_PrEP_indication], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-PrEP users vs. PrEP users by PrEP-indication criteria \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filter_criteria = concatenated_all_indication[['sexualized_drug_use', 'test_syp_freq', 'test_go_freq', 'test_chla_freq','test_hcv_freq', 'condom', 'sex_partner','status']].copy() # PrEP-indication criteria\n",
    "\n",
    "data_filter_criteria = data_filter_criteria.astype('category')\n",
    "columns_filter_criteria = ['sexualized_drug_use', 'test_syp_freq', 'test_go_freq', 'test_chla_freq', 'test_hcv_freq', 'condom', 'sex_partner']\n",
    "categorical_filter_criteria = ['sexualized_drug_use', 'test_syp_freq', 'test_go_freq', 'test_chla_freq', 'test_hcv_freq', 'condom', 'sex_partner']\n",
    "\n",
    "null_value = 'None'\n",
    "for col in categorical_filter_criteria:\n",
    "    data_filter_criteria[col] = data_filter_criteria[col].cat.add_categories(null_value)\n",
    "\n",
    "rename_filter_criteria = {'test_syp_freq': 'Syphilis diagnosis in the last 12 months',\n",
    "                          'test_go_freq': 'Gonorrhoea diagnosis in the last 12 months', \n",
    "                          'test_chla_freq': 'Chlamydia diagnosis in the last 12 months', \n",
    "                          'test_hcv_freq': 'Hepatitis C diagnosis in the last 12 months', \n",
    "                          'sexualized_drug_use': 'Drugs during sex in the last 6 months',\n",
    "                          'condom': 'Condom use',\n",
    "                          'sex_partner': 'Sex partners in the last 6 months'}\n",
    "table_filter_criteria = TableOne(data_filter_criteria, columns = columns_filter_criteria, \n",
    "                                 categorical = categorical_filter_criteria, rename = rename_filter_criteria, \n",
    "                                 groupby = 'status', pval = False, missing = False) \n",
    "\n",
    "print(table_filter_criteria.tabulate(tablefmt = \"fancy grid\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-PrEP users vs. PrEP users (baseline characteristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table_baseline_summary = concatenated_all_indication.copy()\n",
    "\n",
    "columns_baseline_summary  = ['age', 'landbin', 'URBAN_RURAL_AREA', 'HIV_SPECIALIST_DENSITY', 'ABSCHLUSS_TYPE', 'NET_EQUIVALENT_INCOME',\n",
    "                             'SEXLIFE','sex_partner', 'condom', 'sex_freq','sexualized_drug_use', 'drugs_inject', 'sexwork', 'test_syp_freq', 'test_go_freq', 'test_chla_freq', 'test_hcv_freq',\n",
    "                             'sex_with_men','sex_with_women', 'sex_with_nonbinary', 'sti_symptom', 'positive_test_syph', 'positive_test_go', 'positive_test_chla','positive_test_hcv', 'no_positive_test']\n",
    "                            \n",
    "\n",
    "categorical_baseline_summary  =  ['ABSCHLUSS_TYPE', 'SEXLIFE', 'age', 'sex_with_men', 'sex_with_women', 'sex_with_nonbinary', 'sex_partner', 'sex_freq', 'sexualized_drug_use', 'drugs_inject', 'sexwork',\n",
    "                                  'condom', 'sti_symptom', 'positive_test_syph', 'positive_test_go', 'positive_test_chla', 'positive_test_hcv', 'no_positive_test', 'test_syp_freq',\n",
    "                                  'test_go_freq', 'test_chla_freq', 'test_hcv_freq', 'landbin', 'NET_EQUIVALENT_INCOME', 'URBAN_RURAL_AREA', 'HIV_SPECIALIST_DENSITY']\n",
    "\n",
    "\n",
    "rename_baseline_summary = {'age': 'Age', \n",
    "                           'landbin': 'Country of origin', \n",
    "                           'sex_partner': 'Sexpartners in the last 6 months',\n",
    "                           'condom': 'Condom use',\n",
    "                           'ABSCHLUSS_TYPE': 'General school qualification',\n",
    "                           'sex_freq': 'Number of sexual intercourses in the last month',\n",
    "                           'URBAN_RURAL_AREA': 'Urban vs. rural area',\n",
    "                           'HIV_SPECIALIST_DENSITY': 'HIV-specialists density in the federal state of residence',\n",
    "                           'NET_EQUIVALENT_INCOME': 'Monthly net equivalent income',\n",
    "                           'SEXLIFE': 'Satisfaction with sex life'}\n",
    "\n",
    "table_baseline_summary = TableOne(data_table_baseline_summary, columns = columns_baseline_summary, categorical = categorical_baseline_summary, \n",
    "                                       rename = rename_baseline_summary, pval = False, groupby = 'status', missing = False)\n",
    "\n",
    "print(table_baseline_summary.tabulate(tablefmt = \"fancy grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasons for not taking PrEP\n",
    "Reasons for not taking PrEP stratified by survey wave, by PrEP-indication and by low-vs-high HIV-specialists density\n",
    "\n",
    "Test: Chi-Squared (or Fisher's exact)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stratified by low-vs-high HIV-specialists density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group HIV-specialists density into low (0-9 HIV-specialists per 10,000 gay men) and high (10-13 HIV-specialists per 10,000 gay men)\n",
    "no_PrEP_indication['HIV_SPECIALIST_DENSITY_grouped'] = np.where(\n",
    "    no_PrEP_indication['HIV_SPECIALIST_DENSITY'].isin([\"0-2\", \"3-5\", \"6-9\"]), '0-9',\n",
    "    np.where(no_PrEP_indication['HIV_SPECIALIST_DENSITY'] == \"10-13\", '10-13', 'NaN'))\n",
    "\n",
    "no_PrEP_indication['HIV_SPECIALIST_DENSITY_grouped'] = np.where(\n",
    "    no_PrEP_indication['HIV_SPECIALIST_DENSITY'].isna(), 'NaN', no_PrEP_indication['HIV_SPECIALIST_DENSITY_grouped'])\n",
    "\n",
    "no_PrEP_indication['HIV_SPECIALIST_DENSITY_grouped'] = no_PrEP_indication['HIV_SPECIALIST_DENSITY_grouped'].replace('NaN', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reasons = no_PrEP_indication[['reason_no_prep_low_HIV_risk','reason_no_prep_medical_reasons','reason_no_prep_no_doctor','reason_no_prep_talking_sexlife_doctor',\n",
    "                                  'reason_no_prep_effort_too_high','reason_no_prep_daily_pill', 'reason_no_prep_fear_reactions','reason_no_prep_fear_side_effects',\n",
    "                                  'welle', 'URBAN_RURAL_AREA', 'HIV_SPECIALIST_DENSITY_grouped']].copy()\n",
    "\n",
    "columns_reasons = ['reason_no_prep_low_HIV_risk','reason_no_prep_medical_reasons', 'reason_no_prep_no_doctor','reason_no_prep_talking_sexlife_doctor',\n",
    "                   'reason_no_prep_effort_too_high','reason_no_prep_daily_pill', 'reason_no_prep_fear_reactions','reason_no_prep_fear_side_effects']\n",
    "\n",
    "categorical_reasons = ['reason_no_prep_low_HIV_risk','reason_no_prep_medical_reasons','reason_no_prep_no_doctor','reason_no_prep_talking_sexlife_doctor',\n",
    "                       'reason_no_prep_effort_too_high','reason_no_prep_daily_pill', 'reason_no_prep_fear_reactions','reason_no_prep_fear_side_effects']\n",
    "\n",
    "rename_reasons = {'reason_no_prep_low_HIV_risk':  'Personal HIV risk too low', \n",
    "                  'reason_no_prep_medical_reasons': 'Medical reasons', \n",
    "                  'reason_no_prep_no_doctor': 'No doctor prescribing PrEP',\n",
    "                  'reason_no_prep_talking_sexlife_doctor': 'Not discussing my sex life with my doctor',\n",
    "                  'reason_no_prep_effort_too_high': 'Too much effort: doctor visits + regular tests',\n",
    "                  'reason_no_prep_daily_pill': 'Daily pill too strenuous', \n",
    "                  'reason_no_prep_fear_reactions': 'Fear: negative reactions from others/sex partners', \n",
    "                  'reason_no_prep_fear_side_effects': 'Fear: side effects', 'welle': 'survey wave'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no correction for multiple testing \n",
    "table_reason_vs_density = TableOne(data_reasons, columns = columns_reasons , categorical = categorical_reasons, \n",
    "                   rename = rename_reasons, groupby = 'HIV_SPECIALIST_DENSITY_grouped', pval = True, htest_name = True, missing=False)\n",
    "\n",
    "print(table_reason_vs_density.tabulate(tablefmt = \"fancy grid\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correction for multiple testing \n",
    "table_reason_vs_density.htest_table['P-value_corrected'] = multipletests(table_reason_vs_density.htest_table['P-Value'].values.flatten(), method = 'fdr_bh')[1]\n",
    "table_reason_vs_density.htest_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stratified by PrEP indication subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_no_PrEP_indication_subgroups_exclusive = pd.concat([no_PrEP_drug_exclusive, no_PrEP_STI_and_sex_exclusive], ignore_index = True) # exclusive PrEP-indication \n",
    "concatenated_no_PrEP_indication_subgroups_both = pd.concat([no_PrEP_drug_both, no_PrEP_STI_and_sex_exclusive], ignore_index = True) # includes 'both' PrEP-indication "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exclusive groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclusive groups \n",
    "data_reasons_subgroups = concatenated_no_PrEP_indication_subgroups_exclusive[['reason_no_prep_low_HIV_risk','reason_no_prep_medical_reasons', 'reason_no_prep_no_doctor','reason_no_prep_talking_sexlife_doctor',\n",
    "                                                                              'reason_no_prep_effort_too_high','reason_no_prep_daily_pill', 'reason_no_prep_fear_reactions','reason_no_prep_fear_side_effects','INDICATION_SUBGROUP']].copy()\n",
    "\n",
    "\n",
    "columns_reasons_subgroups = ['reason_no_prep_low_HIV_risk','reason_no_prep_medical_reasons','reason_no_prep_no_doctor','reason_no_prep_talking_sexlife_doctor',\n",
    "                             'reason_no_prep_effort_too_high','reason_no_prep_daily_pill','reason_no_prep_fear_reactions','reason_no_prep_fear_side_effects']\n",
    "categorical_reasons_subgroups = ['reason_no_prep_low_HIV_risk','reason_no_prep_medical_reasons','reason_no_prep_no_doctor','reason_no_prep_talking_sexlife_doctor',\n",
    "                                 'reason_no_prep_effort_too_high','reason_no_prep_daily_pill', 'reason_no_prep_fear_reactions', 'reason_no_prep_fear_side_effects']\n",
    "\n",
    "rename_reasons_subgroups = {'reason_no_prep_low_HIV_risk':  'Personal HIV risk too low', \n",
    "                            'reason_no_prep_medical_reasons': 'Medical reasons', \n",
    "                            'reason_no_prep_no_doctor': 'No doctor prescribing PrEP',\n",
    "                            'reason_no_prep_talking_sexlife_doctor': 'Not discussing my sex life with my doctor',\n",
    "                            'reason_no_prep_effort_too_high': 'Too much effort: doctor visits + regular tests',\n",
    "                            'reason_no_prep_daily_pill': 'Daily pill too strenuous', \n",
    "                            'reason_no_prep_fear_reactions': 'Fear: negative reactions from others/sex partners', \n",
    "                            'reason_no_prep_fear_side_effects': 'Fear: side effects', 'welle': 'survey wave'}\n",
    "\n",
    "\n",
    "table_reason_vs_subgroup = TableOne(data_reasons_subgroups, columns = columns_reasons_subgroups , categorical = categorical_reasons_subgroups, \n",
    "                                    rename = rename_reasons_subgroups, groupby = 'INDICATION_SUBGROUP', pval = True, htest_name = True, overall = False)\n",
    "\n",
    "print(table_reason_vs_subgroup.tabulate(tablefmt = \"fancy grid\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_reason_vs_subgroup.htest_table['P-value_corrected'] = multipletests(table_reason_vs_subgroup.htest_table['P-Value'].values.flatten(), method = 'fdr_bh')[1]\n",
    "table_reason_vs_subgroup.htest_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### including both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclusive + both groups \n",
    "data_reasons_subgroups = concatenated_no_PrEP_indication_subgroups_both[['reason_no_prep_low_HIV_risk','reason_no_prep_medical_reasons', 'reason_no_prep_no_doctor','reason_no_prep_talking_sexlife_doctor',\n",
    "                                                                         'reason_no_prep_effort_too_high','reason_no_prep_daily_pill', 'reason_no_prep_fear_reactions','reason_no_prep_fear_side_effects','INDICATION_SUBGROUP']].copy()\n",
    "\n",
    "columns_reasons_subgroups = ['reason_no_prep_low_HIV_risk','reason_no_prep_medical_reasons', 'reason_no_prep_no_doctor','reason_no_prep_talking_sexlife_doctor',\n",
    "                             'reason_no_prep_effort_too_high','reason_no_prep_daily_pill', 'reason_no_prep_fear_reactions','reason_no_prep_fear_side_effects']\n",
    "categorical_reasons_subgroups  = ['reason_no_prep_low_HIV_risk','reason_no_prep_medical_reasons', 'reason_no_prep_no_doctor','reason_no_prep_talking_sexlife_doctor',\n",
    "                                  'reason_no_prep_effort_too_high','reason_no_prep_daily_pill', 'reason_no_prep_fear_reactions','reason_no_prep_fear_side_effects']\n",
    "\n",
    "rename_reasons_subgroups = {'reason_no_prep_low_HIV_risk':  'Personal HIV risk too low', \n",
    "                            'reason_no_prep_medical_reasons': 'Medical reasons', \n",
    "                            'reason_no_prep_no_doctor': 'No doctor prescribing PrEP',\n",
    "                            'reason_no_prep_talking_sexlife_doctor': 'Not discussing my sex life with my doctor',\n",
    "                            'reason_no_prep_effort_too_high': 'Too much effort: doctor visits + regular tests',\n",
    "                            'reason_no_prep_daily_pill': 'Daily pill too strenuous', \n",
    "                            'reason_no_prep_fear_reactions': 'Fear: negative reactions from others/sex partners', \n",
    "                            'reason_no_prep_fear_side_effects': 'Fear: side effects', 'welle': 'survey wave'}\n",
    "\n",
    "table_reason_vs_subgroup = TableOne(data_reasons_subgroups, columns = columns_reasons_subgroups , categorical = categorical_reasons_subgroups, \n",
    "                                    rename = rename_reasons_subgroups, groupby = 'INDICATION_SUBGROUP')\n",
    "\n",
    "print(table_reason_vs_subgroup.tabulate(tablefmt = \"fancy grid\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode target variable\n",
    "# 0: no PrEP, 1: PrEP\n",
    "PrEP_indication['Target'] = 1 \n",
    "no_PrEP_indication['Target'] = 0 \n",
    "\n",
    "# concatenate PrEP and no PrEP (n = 1,458)\n",
    "concatenated_population = pd.concat([PrEP_indication, no_PrEP_indication], ignore_index = True) \n",
    "\n",
    "# drop unnecessary columns (for multivariable analysis)\n",
    "concatenated_population.drop(['welle', 'status', 'reason_no_prep_low_HIV_risk', 'reason_no_prep_medical_reasons', 'reason_no_prep_no_doctor', 'reason_no_prep_talking_sexlife_doctor',\n",
    "                              'reason_no_prep_effort_too_high', 'reason_no_prep_daily_pill', 'reason_no_prep_fear_reactions', 'reason_no_prep_fear_side_effects', 'HIV_SPECIALIST_DENSITY_grouped', 'gendersex'], axis = 1, inplace = True) \n",
    "\n",
    "print(f'Number of Variables (including the target variable): {len(concatenated_population.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary variables --> transform into 0/1 \n",
    "concatenated_population.replace(to_replace = {'ja': 1, 'Ja': 1, 'nein': 0, 'Nein': 0}, inplace=True) \n",
    "\n",
    "# binary w/o missing values (9 variables) --> no further encoding \n",
    "binary_columns = ['sex_with_men','sex_with_women','sex_with_nonbinary', 'positive_test_syph', 'positive_test_go','positive_test_chla','no_positive_test', 'positive_test_hcv', 'landbin']\n",
    "\n",
    "# binary variables w/ missing values (4 variables) --> replace NaN values with 0 \n",
    "already_dummy = ['test_hcv_freq', 'sti_symptom', 'sexualized_drug_use', 'sexwork']\n",
    "\n",
    "for col in already_dummy:\n",
    "    concatenated_population[col] = concatenated_population[col].fillna(0)\n",
    "    \n",
    "# categorical variables (13 variables)\n",
    "categorical_columns = ['ABSCHLUSS_TYPE', 'SEXLIFE', 'age', 'sex_partner', 'sex_freq', 'drugs_inject', 'condom', 'NET_EQUIVALENT_INCOME', 'URBAN_RURAL_AREA', 'HIV_SPECIALIST_DENSITY', 'test_syp_freq', 'test_go_freq', 'test_chla_freq']\n",
    "\n",
    "# encode categorical variables as dummy variables (ignores NaN values)\n",
    "encoded_population = pd.get_dummies(data = concatenated_population, columns = categorical_columns, dtype = int, drop_first = False)\n",
    "\n",
    "# dropping the reference category in the dummy encoded variables (most frequent category)\n",
    "for col in categorical_columns:\n",
    "    most_frequent = concatenated_population[col].value_counts().idxmax() \n",
    "    most_frequent_dummy_col = f\"{col}_{most_frequent}\"\n",
    "    print(f'reference category for \"{col}\" is \"{most_frequent_dummy_col}\"')\n",
    "    encoded_population.drop(most_frequent_dummy_col, axis = 1, inplace = True)\n",
    "\n",
    "encoded_population.to_csv('data/PrApp_encoded.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table showing the encoding of the variables\n",
    "summary_encoding = []\n",
    "\n",
    "for col in concatenated_population.columns:\n",
    "    value_counts = concatenated_population[col].value_counts(dropna=True)\n",
    "    most_frequent = value_counts.idxmax()\n",
    "\n",
    "    categories = []\n",
    "\n",
    "    for cat in value_counts.index:\n",
    "        label = str(cat) \n",
    "        \n",
    "        if (len(value_counts.index) > 2): # only for categorical variables (and not binary variables)\n",
    "            if cat == most_frequent: label += ' (R)'\n",
    "        categories.append(label)\n",
    "\n",
    "    summary_encoding.append({\n",
    "            'Variable': col,\n",
    "            'Categories': ', '.join(categories)})\n",
    "\n",
    "summary_encoding_table = pd.DataFrame(summary_encoding)\n",
    "summary_encoding_table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for different l1 ratios \n",
    "for l1 ratio is [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] \\\n",
    "find optimal regularisation strength using cross-validation \n",
    "\n",
    "CV: Repeated Stratified K-fold (stratified because of class imbalance)\n",
    "\n",
    "\n",
    "Model: Logistic Regression\n",
    "- elastic net with fixed l1-ratio \n",
    "- scoring: ROC AUC\n",
    "- class_weights balanced (because of class imbalance)\n",
    "- CV setting: 15-fold with 10 repeats + random state 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded data\n",
    "encoded_data = pd.read_csv('data/PrApp_encoded.csv', index_col=0) \n",
    "\n",
    "feature_cols = encoded_data.columns.drop('Target') \n",
    "feature_names = encoded_data[feature_cols].columns\n",
    "\n",
    "X = encoded_data[feature_cols].values #  54 features\n",
    "y = encoded_data['Target'].values # 0: No PrEP, 1: PrEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function: hyperparameter tuning for each l1-ratio \n",
    "def parameter_tuning_ROC_AUC(X, y, l1_ratio, state, splits, repeats):\n",
    "      \n",
    "      rng = np.random.RandomState(state) # RandomState instances for models \n",
    "      cv_fold = RepeatedStratifiedKFold(n_splits = splits, n_repeats = repeats, random_state = state)\n",
    "\n",
    "      regularisation_strengths = np.logspace(-1, -2, 100)\n",
    "      logistic_regression_cv = LogisticRegressionCV(penalty = 'elasticnet', \n",
    "                                                   solver = 'saga', \n",
    "                                                   random_state = rng , \n",
    "                                                   max_iter = 1000, \n",
    "                                                   cv = cv_fold, \n",
    "                                                   l1_ratios = [l1_ratio], \n",
    "                                                   Cs = regularisation_strengths,\n",
    "                                                   scoring ='roc_auc',\n",
    "                                                   n_jobs = -1,\n",
    "                                                   class_weight = \"balanced\", \n",
    "                                                   refit = True)\n",
    "      logistic_regression_cv.fit(X, y)\n",
    "      optimal_regulation_strength = logistic_regression_cv.C_[0]\n",
    "      print(f'For l1 ratio = {l1_ratio}')\n",
    "      print(f'the regulation strength with the highest AUC score: {optimal_regulation_strength}')\n",
    "      \n",
    "      mean_auc_scores = logistic_regression_cv.scores_[1].mean(axis = 0)\n",
    "      mean_non_zero_coefficients = np.zeros(len(regularisation_strengths))\n",
    "\n",
    "      results_AUC = []\n",
    "      for C_index, C in enumerate(regularisation_strengths):\n",
    "                  mean_ROC_AUC = mean_auc_scores[C_index][0]\n",
    "                  \n",
    "                  total_non_zero_coefs = 0\n",
    "                  for fold_idx in range(cv_fold.get_n_splits()):\n",
    "                        coefs = logistic_regression_cv.coefs_paths_[1][fold_idx][C_index]\n",
    "                        total_non_zero_coefs += np.sum(coefs != 0)\n",
    "                        \n",
    "                  mean_non_zero_coefs = total_non_zero_coefs / cv_fold.get_n_splits()\n",
    "                  results_AUC.append({\n",
    "                        'C': C,\n",
    "                        'l1_ratio': l1_ratio,\n",
    "                        'mean_ROC_AUC': mean_ROC_AUC,\n",
    "                        'mean_non_zero_coefficients': mean_non_zero_coefs})\n",
    "      \n",
    "      results_AUC  = pd.DataFrame(results_AUC)\n",
    "      return logistic_regression_cv, results_AUC, optimal_regulation_strength\n",
    "\n",
    "\n",
    "\n",
    "# function to get the results dataframe based on the saved logreg model\n",
    "def parameter_tuning_results(logistic_regression_cv, l1_ratio, state, splits, repeats):\n",
    "      \n",
    "      rng = np.random.RandomState(state) # RandomState instances for models \n",
    "      cv_fold = RepeatedStratifiedKFold(n_splits = splits, n_repeats = repeats, random_state = state)\n",
    "\n",
    "      regularisation_strengths = np.logspace(-1, -2, 100)\n",
    "      optimal_regulation_strength = logistic_regression_cv.C_[0]\n",
    "      mean_auc_scores = logistic_regression_cv.scores_[1].mean(axis = 0)\n",
    "      mean_non_zero_coefficients = np.zeros(len(regularisation_strengths))\n",
    "      results_AUC = []\n",
    "      for C_index, C in enumerate(regularisation_strengths):\n",
    "                  mean_ROC_AUC = mean_auc_scores[C_index][0]\n",
    "                  \n",
    "                  total_non_zero_coefs = 0\n",
    "                  for fold_idx in range(cv_fold.get_n_splits()):\n",
    "                        coefs = logistic_regression_cv.coefs_paths_[1][fold_idx][C_index]\n",
    "                        total_non_zero_coefs += np.sum(coefs != 0)\n",
    "                        \n",
    "                  mean_non_zero_coefs = total_non_zero_coefs / cv_fold.get_n_splits()\n",
    "                  results_AUC.append({\n",
    "                        'C': C,\n",
    "                        'l1_ratio': l1_ratio,\n",
    "                        'mean_ROC_AUC': mean_ROC_AUC,\n",
    "                        'mean_non_zero_coefficients': mean_non_zero_coefs})\n",
    "      \n",
    "      results_AUC  = pd.DataFrame(results_AUC)\n",
    "      return results_AUC\n",
    "\n",
    "def plot_cv_model(results_cv, l1_ratio, model):\n",
    "      \n",
    "      C = model.C_[0]\n",
    "\n",
    "      regularisation_strengths = np.logspace(-1, -2, 100)\n",
    "\n",
    "      alphas_elbow = (1 / regularisation_strengths)\n",
    "      \n",
    "      mean_coef = results_cv.loc[results_cv['mean_ROC_AUC'].idxmax()]['mean_non_zero_coefficients']\n",
    "      mean_auc = results_cv.loc[results_cv['mean_ROC_AUC'].idxmax()]['mean_ROC_AUC']\n",
    "      \n",
    "      # plot: mean auc scores vs. regularization strength\n",
    "      plt.figure(figsize=(6, 4))\n",
    "      plt.plot(alphas_elbow , results_cv['mean_ROC_AUC'], \n",
    "               color = \"royalblue\" , linestyle='-')\n",
    "      plt.xlabel('α (regularization strength)')\n",
    "      plt.ylabel('Mean ROC AUC score')\n",
    "      plt.title(f'Parameter tuning: mean AUC scores for different α (l1-ratio = {l1_ratio})')\n",
    "      plt.show()\n",
    "      \n",
    "      print(f'Best AUC: \\n'\n",
    "      f'- Inverse of regularisation strength C: {C} \\n'\n",
    "      f'- Regularisation strength α: {1 / C} \\n'\n",
    "      f'- Mean no. non-zero coefficients: {mean_coef} \\n'\n",
    "      f'- Mean ROC AUC score: {mean_auc}')\n",
    "      \n",
    "      # plot: regularization path\n",
    "      mean_coefs_value = np.mean(model.coefs_paths_[1][:], axis=(0))\n",
    "      plt.figure(figsize=(6, 4))\n",
    "      for i in range(mean_coefs_value.shape[2]):\n",
    "            plt.plot(np.log10(alphas_elbow), \n",
    "            mean_coefs_value[:, 0, i])\n",
    "      plt.xlabel('log(α)')\n",
    "      plt.ylabel('Mean coefficient value')\n",
    "      plt.title(f'Regularization Path: l1-ratio = {l1_ratio}')\n",
    "      plt.show()\n",
    "      \n",
    "      if l1_ratio == 0.0: \n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.scatter(alphas_elbow, results_cv['mean_non_zero_coefficients'],\n",
    "                  linewidths = 1, alpha = .7,\n",
    "                  s = 50,\n",
    "                  c = results_cv['mean_ROC_AUC'])\n",
    "            plt.ylabel('Mean no. non-zero coefficients')\n",
    "            plt.xlabel('α (regularization strength)')\n",
    "            plt.colorbar(label = 'Mean ROC AUC score')\n",
    "            plt.show()\n",
    "            \n",
    "            print('no elbow point')\n",
    "      \n",
    "      else:\n",
    "            # plot: elbow plot \n",
    "            kf_auc_vs_coef  = KneeFinder(results_cv['mean_ROC_AUC'], results_cv['mean_non_zero_coefficients'])\n",
    "            knee_x_auc_vs_coef, knee_y_auc_vs_coef = kf_auc_vs_coef.find_knee() \n",
    "      \n",
    "            idx_point_coef_vs_auc =  np.where(results_cv['mean_ROC_AUC'].values == knee_x_auc_vs_coef)[0] \n",
    "            alpha_coef_vs_auc = alphas_elbow[idx_point_coef_vs_auc][0]\n",
    "\n",
    "      \n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.plot(results_cv['mean_ROC_AUC'], results_cv['mean_non_zero_coefficients'], \n",
    "                  color = \"royalblue\" , linestyle='-')\n",
    "            plt.xlabel('Mean ROC AUC Score')\n",
    "            plt.ylabel('Mean no. non-zero coefficients')\n",
    "            plt.axvline(x = knee_x_auc_vs_coef, color='crimson', linestyle='--', label = f\" elbow point \")\n",
    "            plt.legend()\n",
    "            plt.title(f'Mean ROC AUC l1-ratio = {l1_ratio}')\n",
    "            plt.show()\n",
    "      \n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.scatter(alphas_elbow, results_cv['mean_non_zero_coefficients'],\n",
    "                        linewidths = 1, alpha = .7, s = 50, c = results_cv['mean_ROC_AUC'])\n",
    "            plt.ylabel('Mean no. non-zero coefficients')\n",
    "            plt.xlabel('α (regularization strength)')\n",
    "            plt.colorbar(label = 'Mean ROC AUC score')\n",
    "            plt.axvline(x = alpha_coef_vs_auc , color='crimson', linestyle='--', \n",
    "                        label = f\"elbow point at α = {round(alpha_coef_vs_auc , 2)}\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            print(f'Elbow point: \\n'\n",
    "                  f'- Inverse of regularisation strength C: {1 / alpha_coef_vs_auc} \\n'\n",
    "                  f'- Regularisation strength α: {alpha_coef_vs_auc} \\n'\n",
    "                  f'- Mean no. non-zero coefficients: {knee_x_auc_vs_coef} \\n'\n",
    "                  f'- Mean ROC AUC score: {knee_y_auc_vs_coef}')\n",
    "            \n",
    "            return alpha_coef_vs_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1-ratio = 0 (L2 regularization)\n",
    "\n",
    "elbow point: not existing because the mean number of mean non-zero coefficients is constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_0, results_0, optimal_C_0 = parameter_tuning_ROC_AUC(X, y, l1_ratio = 0.0, state = 1, splits = 15, repeats = 10)\n",
    "dump(logreg_0, 'results/parameter_tuning_models/logreg_0.joblib') \n",
    "\n",
    "logreg_0 = load('results/parameter_tuning_models/logreg_0.joblib') \n",
    "results_0 = parameter_tuning_results(logreg_0, l1_ratio = 0.0, state = 1, splits = 15, repeats = 10) \n",
    "plot_cv_model(results_0, l1_ratio = 0.0, model = logreg_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1-ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_0_1, results_0_1, optimal_C_0_1 = parameter_tuning_ROC_AUC(X, y, l1_ratio = 0.1, state = 1, splits = 15, repeats = 10)\n",
    "dump(logreg_0_1, 'results/parameter_tuning_models/logreg_0_1.joblib')\n",
    "\n",
    "logreg_0_1 = load('results/parameter_tuning_models/logreg_0_1.joblib')\n",
    "results_0_1 = parameter_tuning_results(logreg_0_1, l1_ratio = 0.1, state = 1, splits = 15, repeats = 10)\n",
    "alpha_elbow_0_1 = plot_cv_model(results_0_1, l1_ratio = 0.1, model = logreg_0_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1-ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_0_2, results_0_2, optimal_C_0_2 = parameter_tuning_ROC_AUC(X, y, l1_ratio = 0.2, state = 1, splits = 15, repeats = 10)\n",
    "dump(logreg_0_2, 'results/parameter_tuning_models/logreg_0_2.joblib') \n",
    "\n",
    "logreg_0_2 = load('results/parameter_tuning_models/logreg_0_2.joblib') \n",
    "results_0_2 = parameter_tuning_results(logreg_0_2, l1_ratio = 0.2, state = 1, splits = 15, repeats = 10) \n",
    "alpha_elbow_0_2 = plot_cv_model(results_0_2, l1_ratio = 0.2, model = logreg_0_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1-ratio = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_0_3, results_0_3, optimal_C_0_3 = parameter_tuning_ROC_AUC(X, y, l1_ratio = 0.3, state = 1, splits = 15, repeats = 10)\n",
    "dump(logreg_0_3, 'results/parameter_tuning_models/logreg_0_3.joblib')\n",
    "\n",
    "logreg_0_3 = load('results/parameter_tuning_models/logreg_0_3.joblib') \n",
    "results_0_3 = parameter_tuning_results(logreg_0_3, l1_ratio = 0.3, state = 1, splits = 15, repeats = 10) \n",
    "alpha_elbow_0_3 = plot_cv_model(results_0_3, l1_ratio = 0.3, model = logreg_0_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1-ratio = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_0_4, results_0_4, optimal_C_0_4 = parameter_tuning_ROC_AUC(X, y, l1_ratio = 0.4, \n",
    "                                                                                    state = 1, splits = 15, repeats = 10)\n",
    "dump(logreg_0_4, 'results/parameter_tuning_models/logreg_0_4.joblib')\n",
    "\n",
    "logreg_0_4 = load('results/parameter_tuning_models/logreg_0_4.joblib') \n",
    "results_0_4 = parameter_tuning_results(logreg_0_4, l1_ratio = 0.4, state = 1, splits = 15, repeats = 10) \n",
    "alpha_elbow_0_4 = plot_cv_model(results_0_4, l1_ratio = 0.4, model = logreg_0_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1-ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_0_5, results_0_5, optimal_C_0_5 = parameter_tuning_ROC_AUC(X, y, l1_ratio = 0.5, state = 1, splits = 15, repeats = 10)\n",
    "dump(logreg_0_5, 'results/parameter_tuning_models/logreg_0_5.joblib')\n",
    "\n",
    "logreg_0_5 = load('results/parameter_tuning_models/logreg_0_5.joblib') \n",
    "results_0_5 = parameter_tuning_results(logreg_0_5, l1_ratio = 0.5, state = 1, splits = 15, repeats = 10) \n",
    "alpha_elbow_0_5 = plot_cv_model(results_0_5, l1_ratio = 0.5, model = logreg_0_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1-ratio = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_0_6, results_0_6, optimal_C_0_6 = parameter_tuning_ROC_AUC(X, y, l1_ratio = 0.6, \n",
    "                                                                                    state = 1, splits = 15, repeats = 10)\n",
    "dump(logreg_0_6, 'results/parameter_tuning_models/logreg_0_6.joblib') \n",
    "\n",
    "logreg_0_6 = load('results/parameter_tuning_models/logreg_0_6.joblib') \n",
    "results_0_6 = parameter_tuning_results(logreg_0_6, l1_ratio = 0.6, state = 1, splits = 15, repeats = 10) \n",
    "alpha_elbow_0_6 = plot_cv_model(results_0_6, l1_ratio = 0.6, model = logreg_0_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1-ratio = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_0_7, results_0_7, optimal_C_0_7 = parameter_tuning_ROC_AUC(X, y, l1_ratio = 0.7, \n",
    "                                                                                    state = 1, splits = 15, repeats = 10)\n",
    "dump(logreg_0_7, 'results/parameter_tuning_models/logreg_0_7.joblib')\n",
    "\n",
    "logreg_0_7 = load('results/parameter_tuning_models/logreg_0_7.joblib') \n",
    "results_0_7 = parameter_tuning_results(logreg_0_7, l1_ratio = 0.7, state = 1, splits = 15, repeats = 10) \n",
    "alpha_elbow_0_7 = plot_cv_model(results_0_7, l1_ratio = 0.7, model = logreg_0_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1-ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_0_8, results_0_8, optimal_C_0_8 = parameter_tuning_ROC_AUC(X, y, l1_ratio = 0.8, \n",
    "                                                                                    state = 1, splits = 15, repeats = 10)\n",
    "dump(logreg_0_8, 'results/parameter_tuning_models/logreg_0_8.joblib')\n",
    "\n",
    "logreg_0_8 = load('results/parameter_tuning_models/logreg_0_8.joblib') \n",
    "results_0_8 = parameter_tuning_results(logreg_0_8, l1_ratio = 0.8, state = 1, splits = 15, repeats = 10) \n",
    "alpha_elbow_0_8 = plot_cv_model(results_0_8, l1_ratio = 0.8, model = logreg_0_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1-ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_0_9, results_0_9, optimal_C_0_9 = parameter_tuning_ROC_AUC(X, y, l1_ratio = 0.9, \n",
    "                                                                                    state = 1, splits = 15, repeats = 10)\n",
    "dump(logreg_0_9, 'results/parameter_tuning_models/logreg_0_9.joblib')\n",
    "\n",
    "logreg_0_9 = load('results/parameter_tuning_models/logreg_0_9.joblib') \n",
    "results_0_9 = parameter_tuning_results(logreg_0_9, l1_ratio = 0.9, state = 1, splits = 15, repeats = 10) \n",
    "alpha_elbow_0_9 = plot_cv_model(results_0_9, l1_ratio = 0.9, model = logreg_0_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1-ratio = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_1_0, results_1_0, optimal_C_1_0 = parameter_tuning_ROC_AUC(X, y, l1_ratio = 1.0, \n",
    "                                                                                    state = 1, splits = 15, repeats = 10)\n",
    "dump(logreg_1_0, 'results/parameter_tuning_models/logreg_1_0.joblib')\n",
    "\n",
    "logreg_1_0 = load('results/parameter_tuning_models/logreg_1_0.joblib') \n",
    "results_1_0 = parameter_tuning_results(logreg_1_0, l1_ratio = 1.0, state = 1, splits = 15, repeats = 10) \n",
    "alpha_elbow_1_0 = plot_cv_model(results_1_0, l1_ratio = 1.0, model = logreg_1_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "for each l1_ratio a model is fitted with the regularisation strength from elbow point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "\n",
    "# models one for each l1-ratio fitted with regularisation strength from elbow point \n",
    "model_0_1 = LogisticRegression(\n",
    "            penalty= 'elasticnet',\n",
    "            C = 1 / alpha_elbow_0_1,\n",
    "            l1_ratio = 0.1,\n",
    "            max_iter = 1000,\n",
    "            class_weight = \"balanced\",\n",
    "            random_state = rng, \n",
    "            solver ='saga')\n",
    "      \n",
    "model_0_1.fit(X, y)\n",
    "\n",
    "dump(model_0_1, 'results/parameter_tuning_models/logreg_0_1_elbow.joblib') \n",
    "\n",
    "model_0_2 = LogisticRegression(\n",
    "            penalty= 'elasticnet',\n",
    "            C = 1 / alpha_elbow_0_2,\n",
    "            l1_ratio = 0.2,\n",
    "            max_iter = 1000,\n",
    "            class_weight = \"balanced\",\n",
    "            random_state = rng, \n",
    "            solver='saga')\n",
    "      \n",
    "model_0_2.fit(X, y)\n",
    "dump(model_0_2, 'results/parameter_tuning_models/logreg_0_2_elbow.joblib')\n",
    "\n",
    "model_0_3 = LogisticRegression(\n",
    "            penalty= 'elasticnet',\n",
    "            C = 1 / alpha_elbow_0_3,\n",
    "            l1_ratio = 0.3,\n",
    "            max_iter = 1000,\n",
    "            class_weight = \"balanced\",\n",
    "            random_state = rng, \n",
    "            solver='saga')\n",
    "      \n",
    "model_0_3.fit(X, y)\n",
    "dump(model_0_3, 'results/parameter_tuning_models/logreg_0_3_elbow.joblib')\n",
    "\n",
    "model_0_4 = LogisticRegression(\n",
    "            penalty= 'elasticnet',\n",
    "            C = 1 / alpha_elbow_0_4,\n",
    "            l1_ratio = 0.4,\n",
    "            max_iter = 1000,\n",
    "            class_weight = \"balanced\",\n",
    "            random_state = rng, \n",
    "            solver='saga')\n",
    "      \n",
    "model_0_4.fit(X, y)\n",
    "dump(model_0_4, 'results/parameter_tuning_models/logreg_0_4_elbow.joblib')\n",
    "\n",
    "model_0_5 = LogisticRegression(\n",
    "            penalty= 'elasticnet',\n",
    "            C = 1 / alpha_elbow_0_5,\n",
    "            l1_ratio = 0.5,\n",
    "            max_iter = 1000,\n",
    "            class_weight = \"balanced\",\n",
    "            random_state = rng, \n",
    "            solver='saga')\n",
    "      \n",
    "model_0_5.fit(X, y)\n",
    "dump(model_0_5, 'results/parameter_tuning_models/logreg_0_5_elbow.joblib')\n",
    "\n",
    "model_0_6 = LogisticRegression(\n",
    "            penalty= 'elasticnet',\n",
    "            C = 1 / alpha_elbow_0_6,\n",
    "            l1_ratio = 0.6,\n",
    "            max_iter = 1000,\n",
    "            class_weight = \"balanced\",\n",
    "            random_state = rng, \n",
    "            solver='saga')\n",
    "      \n",
    "model_0_6.fit(X, y)\n",
    "dump(model_0_6, 'results/parameter_tuning_models/logreg_0_6_elbow.joblib')\n",
    "\n",
    "model_0_7 = LogisticRegression(\n",
    "            penalty= 'elasticnet',\n",
    "            C = 1 / alpha_elbow_0_7,\n",
    "            l1_ratio = 0.7,\n",
    "            max_iter = 1000,\n",
    "            class_weight = \"balanced\",\n",
    "            random_state = rng, \n",
    "            solver='saga')\n",
    "      \n",
    "model_0_7.fit(X, y)\n",
    "dump(model_0_7, 'results/parameter_tuning_models/logreg_0_7_elbow.joblib')\n",
    "\n",
    "model_0_8 = LogisticRegression(\n",
    "            penalty= 'elasticnet',\n",
    "            C = 1 / alpha_elbow_0_8,\n",
    "            l1_ratio = 0.8,\n",
    "            max_iter = 1000,\n",
    "            class_weight = \"balanced\",\n",
    "            random_state = rng, \n",
    "            solver='saga')\n",
    "      \n",
    "model_0_8.fit(X, y)\n",
    "dump(model_0_8, 'results/parameter_tuning_models/logreg_0_8_elbow.joblib')\n",
    "\n",
    "model_0_9 = LogisticRegression(\n",
    "            penalty= 'elasticnet',\n",
    "            C = 1 / alpha_elbow_0_9,\n",
    "            l1_ratio = 0.9,\n",
    "            max_iter = 1000,\n",
    "            class_weight = \"balanced\",\n",
    "            random_state = rng, \n",
    "            solver='saga')\n",
    "      \n",
    "model_0_9.fit(X, y)\n",
    "dump(model_0_9, 'results/parameter_tuning_models/logreg_0_9_elbow.joblib')\n",
    "\n",
    "model_1_0 = LogisticRegression(\n",
    "            penalty= 'elasticnet',\n",
    "            C = 1 / alpha_elbow_1_0,\n",
    "            l1_ratio = 1.0,\n",
    "            max_iter = 1000,\n",
    "            class_weight = \"balanced\",\n",
    "            random_state = rng, \n",
    "            solver='saga')\n",
    "      \n",
    "model_1_0.fit(X, y)\n",
    "dump(model_1_0, 'results/parameter_tuning_models/logreg_1_0_elbow.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models one for each l1-ratio fitted with regularisation strength from elbow point\n",
    "model_0_1 = load('results/parameter_tuning_models/logreg_0_1_elbow.joblib') \n",
    "model_0_2 = load('results/parameter_tuning_models/logreg_0_2_elbow.joblib') \n",
    "model_0_3 = load('results/parameter_tuning_models/logreg_0_3_elbow.joblib') \n",
    "model_0_4 = load('results/parameter_tuning_models/logreg_0_4_elbow.joblib') \n",
    "model_0_5 = load('results/parameter_tuning_models/logreg_0_5_elbow.joblib') \n",
    "model_0_6 = load('results/parameter_tuning_models/logreg_0_6_elbow.joblib') \n",
    "model_0_7 = load('results/parameter_tuning_models/logreg_0_7_elbow.joblib') \n",
    "model_0_8 = load('results/parameter_tuning_models/logreg_0_8_elbow.joblib') \n",
    "model_0_9 = load('results/parameter_tuning_models/logreg_0_9_elbow.joblib') \n",
    "model_1_0 = load('results/parameter_tuning_models/logreg_1_0_elbow.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aic(y_true, y_pred_proba, num_params):\n",
    "    # log_loss: negative log-likelihood of a logistic model (- log_loss: log-likelihood) ln(L)\n",
    "    # normalize = False: sum of the per-sample losses\n",
    "    log_like = - log_loss(y_true, y_pred_proba, normalize = False)\n",
    "    aic = 2 * num_params - 2 * log_like\n",
    "    return aic\n",
    "\n",
    "def calculate_bic(y_true, y_pred_proba, num_params):\n",
    "    log_like = - log_loss(y_true, y_pred_proba, normalize = False)\n",
    "    n = len(y_true) # no. observations\n",
    "    bic = - 2 * log_like + np.log(n) * num_params \n",
    "    return bic\n",
    "\n",
    "\n",
    "def model_ICs(model, l1, optimal_C, modus):\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    num_params = len(model.coef_[0]) + 1 # + intercept \n",
    "    aic = calculate_aic(y, y_pred_proba, num_params)\n",
    "    bic = calculate_bic(y, y_pred_proba, num_params)\n",
    "    \n",
    "    c = model.coef_.ravel()\n",
    "    non_zero_index = np.nonzero(c)[0]\n",
    "    non_zero_coefficients = len(non_zero_index)\n",
    "    \n",
    "    auc = roc_auc_score(y, y_pred_proba)\n",
    "    \n",
    "    results_IC = pd.DataFrame({\n",
    "        'l1-ratio': [l1],\n",
    "        'modus': [modus],\n",
    "        'inverse regularisation strength C': [optimal_C],\n",
    "        'regularisation strength alpha': [1/optimal_C],\n",
    "        'ROC_AUC': [auc],\n",
    "        'non_zero_coefficients': [non_zero_coefficients],\n",
    "        'AIC': [aic],\n",
    "        'BIC': [bic]\n",
    "    })\n",
    "      \n",
    "    return results_IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitted models for each l1-ratio with regularisation strength from elbow point detection\n",
    "results_best_elbow_0_1 = model_ICs(model_0_1, l1 = 0.1, optimal_C = 1 / alpha_elbow_0_1, modus = \"elbow point\")\n",
    "results_best_elbow_0_2 = model_ICs(model_0_2, l1 = 0.2, optimal_C = 1 / alpha_elbow_0_2, modus = \"elbow point\")\n",
    "results_best_elbow_0_3 = model_ICs(model_0_3, l1 = 0.3, optimal_C = 1 / alpha_elbow_0_3, modus = \"elbow point\")\n",
    "results_best_elbow_0_4 = model_ICs(model_0_4, l1 = 0.4, optimal_C = 1 / alpha_elbow_0_4, modus = \"elbow point\")\n",
    "results_best_elbow_0_5 = model_ICs(model_0_5, l1 = 0.5, optimal_C = 1 / alpha_elbow_0_5, modus = \"elbow point\")\n",
    "results_best_elbow_0_6 = model_ICs(model_0_6, l1 = 0.6, optimal_C = 1 / alpha_elbow_0_6, modus = \"elbow point\")\n",
    "results_best_elbow_0_7 = model_ICs(model_0_7, l1 = 0.7, optimal_C = 1 / alpha_elbow_0_7, modus = \"elbow point\")\n",
    "results_best_elbow_0_8 = model_ICs(model_0_8, l1 = 0.8, optimal_C = 1 / alpha_elbow_0_8, modus = \"elbow point\")\n",
    "results_best_elbow_0_9 = model_ICs(model_0_9, l1 = 0.9, optimal_C = 1 / alpha_elbow_0_9, modus = \"elbow point\")\n",
    "results_best_elbow_1_0 = model_ICs(model_1_0, l1 = 1.0, optimal_C = 1 / alpha_elbow_1_0, modus = \"elbow point\")\n",
    "\n",
    "results_model_selection_best_elbow = pd.concat([results_best_elbow_0_1, results_best_elbow_0_2, \n",
    "                                                results_best_elbow_0_3, results_best_elbow_0_4, results_best_elbow_0_5, results_best_elbow_0_6, results_best_elbow_0_7,\n",
    "                                                results_best_elbow_0_8, results_best_elbow_0_9, results_best_elbow_1_0], axis=0, ignore_index=True)  \n",
    "\n",
    "results_model_selection_best_elbow_rounded = results_model_selection_best_elbow.round(2)\n",
    "results_model_selection_best_elbow_rounded               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot: AIC vs. sparsity (number of non-zero coefficients)\n",
    "# we want: low AIC and low no. of  -> based on your data decide which model to select\n",
    "\n",
    "# trained models with regularisation strength from elbow point\n",
    "plt.figure(figsize=(8, 4))\n",
    "scatter_elbow = sns.scatterplot(data = results_model_selection_best_elbow, x ='non_zero_coefficients', \n",
    "                                y ='AIC', hue ='l1-ratio', palette = sns.color_palette(\"magma\", 10), s = 50)\n",
    "for i in range(results_model_selection_best_elbow.shape[0]): # add labels to scatter points \n",
    "    plt.text(\n",
    "        results_model_selection_best_elbow['non_zero_coefficients'].iloc[i] + 0.6, \n",
    "        results_model_selection_best_elbow['AIC'].iloc[i] - 4,\n",
    "        f'{results_model_selection_best_elbow[\"l1-ratio\"].iloc[i]:.1f}',  \n",
    "        fontsize = 10,\n",
    "        ha = 'left')\n",
    "handles, labels = scatter_elbow.get_legend_handles_labels() # add alpha to legend\n",
    "\n",
    "custom_labels = [\n",
    "    f'l1-ratio: {results_model_selection_best_elbow[\"l1-ratio\"].iloc[i]:.1f}, '\n",
    "    f' α: {results_model_selection_best_elbow[\"regularisation strength alpha\"].iloc[i]:.2f}'\n",
    "    for i in range(len(labels) )\n",
    "]\n",
    "\n",
    "plt.legend(handles = handles[:], labels = custom_labels, title = 'Model parameters', title_fontsize='12', loc='center left',  bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Number of non-zero coefficients')\n",
    "plt.ylabel('AIC')\n",
    "plt.title('Model selection: AIC vs. sparsity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariable analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariable regression model\n",
    "selected model from model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = load('results/parameter_tuning_models/logreg_0_4_elbow.joblib')  # based on model selection \n",
    "final_model.fit(X, y)\n",
    "\n",
    "# final coefficients\n",
    "final_coefficients = final_model.coef_\n",
    "final_non_zero_coefficients = final_coefficients[np.nonzero(final_coefficients)]\n",
    "final_non_zero_feature_names = feature_names[np.nonzero(final_coefficients)[1]]\n",
    "final_non_zero_coefficients_df = pd.DataFrame({'Feature': final_non_zero_feature_names,\n",
    "                                               'Coefficient': final_non_zero_coefficients})\n",
    "\n",
    "final_coefficients_df = pd.DataFrame({'Feature': feature_names,\n",
    "                                      'Coefficient': final_coefficients[0]})\n",
    "\n",
    "# optimal classification threshold \n",
    "y_proba = final_model.predict_proba(X)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y, y_proba)\n",
    "optimal_threshold_index  = np.argmax(tpr  - fpr)\n",
    "optimal_threshold  = thresholds[optimal_threshold_index]\n",
    "y_pred_adjusted = (y_proba >= optimal_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-zero regression coefficients (scatterplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot: non-zero coefficients\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.scatter(final_non_zero_coefficients_df['Coefficient'], final_non_zero_coefficients_df['Feature'], color='b')\n",
    "plt.xlabel('Regression coefficient')\n",
    "plt.axvline(0, color='black', linestyle='--')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Final Model: Non-zero Regression Coefficients')\n",
    "\n",
    "for i, row in final_non_zero_coefficients_df.iterrows():\n",
    "    plt.text(\n",
    "        x=row['Coefficient'] + 0.03 * max(abs(final_non_zero_coefficients_df['Coefficient'])), \n",
    "        y=row['Feature'],\n",
    "        s=f\"{row['Coefficient']:.2f}\",\n",
    "        va='center',\n",
    "        fontsize=10,\n",
    "        color='black'\n",
    "    )\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-zero coefficients\n",
    "round(final_non_zero_coefficients_df,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimal classification threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve with optimal classification threshold\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr, label='ROC Curve', color = 'black')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "plt.scatter(fpr[optimal_threshold_index], tpr[optimal_threshold_index], \n",
    "            marker='o', color='red', label=f'Optimal threshold: {optimal_threshold:.2f}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_classification_threshold = pd.DataFrame({\n",
    "    'True Label': y,\n",
    "    'Predicted Probability': y_proba,\n",
    "    'Group': np.where(y == 1, 'PrEP', 'no PrEP')})\n",
    "\n",
    "colors_group = {'PrEP': 'seagreen', 'no PrEP': 'orange'}\n",
    "\n",
    "# violin plot: predicted probabilities with optimal classification threshold\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.violinplot(x = 'True Label', y ='Predicted Probability', data = adjusted_classification_threshold, palette = colors_group, hue = 'Group')\n",
    "plt.axhline(optimal_threshold, color='red', linestyle='--', label=f'optimal threshold: {optimal_threshold:.2f}')\n",
    "plt.xlabel('True label')\n",
    "plt.xticks([0, 1], labels = ['no PrEP', 'PrEP'])\n",
    "plt.ylabel('Predicted probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping \n",
    "10,000 stratified bootstrapped sampled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = load('results/parameter_tuning_models/logreg_0_4_elbow.joblib') # load final selected model \n",
    "rng = np.random.RandomState(1)\n",
    "bootstrapped_coefficients = []\n",
    "\n",
    "for _ in range(10000):\n",
    "    X_bootstrap, y_bootstrap = resample(X, y, \n",
    "                                        random_state = rng, \n",
    "                                        stratify = y,\n",
    "                                        replace = True)\n",
    "    final_model.fit(X_bootstrap, y_bootstrap)\n",
    "    bootstrapped_coefficients.append(final_model.coef_[0])\n",
    "    \n",
    "bootstrapped_coefficients_df = pd.DataFrame(bootstrapped_coefficients, columns = feature_names) # all bootstrapped regression coefficients\n",
    "non_zero_bootstrapped_coefficients_df = bootstrapped_coefficients_df.loc[:, (bootstrapped_coefficients_df != 0).any(axis=0)] # all bootstrapped non-zeroregression coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-parametric bootstrap: percentile confidence interval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_lower = np.percentile(np.array(bootstrapped_coefficients_df), 2.5, axis = 0) # lower 95% confidence interval\n",
    "CI_upper = np.percentile(np.array(bootstrapped_coefficients_df), 97.5, axis = 0) # upper 95% confidence interval\n",
    "\n",
    "multivariable_coef_results = final_coefficients_df.copy()\n",
    "multivariable_coef_results['lower 95% CI'] = CI_lower\n",
    "multivariable_coef_results['upper 95% CI'] = CI_upper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_coefs = (CI_lower > 0) | (CI_upper < 0) # significant coefficients based on confidence intervals (check: is 0 included in the CI)\n",
    "\n",
    "significant_from_CI_df = pd.DataFrame({\n",
    "    'Feature': bootstrapped_coefficients_df.columns,\n",
    "    'Lower 95% CI': CI_lower,\n",
    "    'Upper 95% CI': CI_upper,\n",
    "    'Significant': significant_coefs\n",
    "})\n",
    "\n",
    "significant_from_CI_df.loc[significant_from_CI_df['Significant'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-parametric bootstrap: hypothesis test \n",
    "\n",
    "H0: β = 0 \n",
    "\n",
    "H1: β ≠ 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_samples = 10000\n",
    "p_values_features = {}\n",
    "\n",
    "for feature in bootstrapped_coefficients_df.columns:\n",
    "    bst_coefs = bootstrapped_coefficients_df[feature] # bootstrapped coefficients for the feature\n",
    "\n",
    "    greater_than_zero = np.sum(bst_coefs >= 0) / bootstrap_samples # sum of bootstrapped coefficients >= zero / bootstrap samples\n",
    "    less_than_zero = np.sum(bst_coefs <= 0) / bootstrap_samples # sum of bootstrapped coefficients <= zero / bootstrap samples\n",
    "    \n",
    "    p_value = min(greater_than_zero, less_than_zero)\n",
    "    p_values_features[feature] = p_value\n",
    "\n",
    "\n",
    "p_values_features_df = pd.DataFrame([p_values_features]).T\n",
    "p_values_features_df.rename(columns = {0: 'p_values'}, inplace = True)\n",
    "\n",
    "\n",
    "multivariable_coef_results['p_values'] = p_values_features_df['p_values'].values\n",
    "\n",
    "\n",
    "p_values_features_df.sort_values(by = 'p_values', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_2_decimal = ['Coefficient', 'lower 95% CI', 'upper 95% CI']\n",
    "columns_3_decimal = ['p_values']\n",
    "\n",
    "multivariable_coef_results_rounded = multivariable_coef_results.copy()\n",
    "multivariable_coef_results_rounded[columns_2_decimal] = multivariable_coef_results_rounded[columns_2_decimal].round(2)\n",
    "multivariable_coef_results_rounded[columns_3_decimal] = multivariable_coef_results_rounded[columns_3_decimal].round(3)\n",
    "\n",
    "multivariable_coef_results_rounded.to_csv('results/multivariable_analysis_result.csv', index = True)\n",
    "\n",
    "multivariable_coef_results_rounded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### boxplot significant variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 significant features\n",
    "significant_features_multivariable = multivariable_coef_results.loc[multivariable_coef_results['p_values'] < 0.05]['Feature']\n",
    "significant_bst_coef = bootstrapped_coefficients_df[significant_features_multivariable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variables = ['sexualized_drug_use', 'positive_test_chla', 'SEXLIFE_Discontent', 'SEXLIFE_Sex doesnt matter right now',\n",
    "                  'age_18-29 yrs', 'age_40-49 yrs', 'age_50-80 yrs', 'sex_partner_0', 'sex_partner_1', 'sex_partner_11-20',\n",
    "                  'sex_partner_2-3', 'sex_partner_4-5', 'sex_partner_6-10',\n",
    "                  'condom_0%', 'condom_50%', 'condom_75%', 'condom_>95%',\n",
    "                  'NET_EQUIVALENT_INCOME_1 bis <1000', 'NET_EQUIVALENT_INCOME_1000 bis <2000', 'NET_EQUIVALENT_INCOME_3000 bis <4000',\n",
    "                  'NET_EQUIVALENT_INCOME_4000 bis <5000', 'NET_EQUIVALENT_INCOME_5000+',\n",
    "                  'HIV_SPECIALIST_DENSITY_0-2', 'HIV_SPECIALIST_DENSITY_10-13', 'HIV_SPECIALIST_DENSITY_3-5']\n",
    "\n",
    "plot_bst_strata = bootstrapped_coefficients_df[plot_variables]\n",
    "\n",
    "# add reference categories as 0 \n",
    "plot_bst_strata['30-39 years (R)'] = 0\n",
    "plot_bst_strata['6-9 (R)'] = 0\n",
    "plot_bst_strata['2000 - <3000€ (R)'] = 0\n",
    "plot_bst_strata['Content (R)'] = 0\n",
    "plot_bst_strata[r'$\\geq$ 20 (R)'] = 0\n",
    "plot_bst_strata['25% (R)'] = 0\n",
    "\n",
    "plot_bst_strata_renamed = plot_bst_strata.rename(columns = {'age_18-29 yrs': r'$\\mathbf{Age}$ 18-29 years',\n",
    "                                                            'age_40-49 yrs': '40-49 years', 'age_50-80 yrs': '50-80 years',\n",
    "                                                            'HIV_SPECIALIST_DENSITY_0-2': r'$\\mathbf{HIV-specialists\\ density}$ 0-2', \n",
    "                                                            'HIV_SPECIALIST_DENSITY_3-5': '3-5', 'HIV_SPECIALIST_DENSITY_10-13': '10-13',\n",
    "                                                            'NET_EQUIVALENT_INCOME_1 bis <1000': r'$\\mathbf{Monthly\\ net-equivalent\\ income}$ 1 - <1000€',\n",
    "                                                            'NET_EQUIVALENT_INCOME_1000 bis <2000': '1000 - <2000€',\n",
    "                                                            'NET_EQUIVALENT_INCOME_3000 bis <4000': '3000 - <4000€',\n",
    "                                                            'NET_EQUIVALENT_INCOME_4000 bis <5000': '4000 - <5000€', \n",
    "                                                            'NET_EQUIVALENT_INCOME_5000+': r'$\\geq$ 5000€',\n",
    "                                                            'SEXLIFE_Discontent': r'$\\mathbf{Satisfaction\\ with\\ sex\\ life}$ Discontent',\n",
    "                                                            'SEXLIFE_Sex doesnt matter right now': \"Sex doesn't matter right now\",\n",
    "                                                            'sex_partner_0': r'$\\mathbf{Sex\\ partners\\ in\\ the\\ last\\ 6\\ months}$ 0',\n",
    "                                                            'sex_partner_1': '1',\n",
    "                                                            'sex_partner_2-3': '2-3',\n",
    "                                                            'sex_partner_4-5': '4-5',\n",
    "                                                            'sex_partner_6-10': '6-10',\n",
    "                                                            'sex_partner_11-20': '11-20',\n",
    "                                                            'condom_0%': r'$\\mathbf{Condom\\ use}$ 0%',\n",
    "                                                            'condom_50%': '50%',\n",
    "                                                            'condom_75%': '75%',\n",
    "                                                            'condom_>95%': '>95%',\n",
    "                                                            'sexualized_drug_use': r'$\\mathbf{Sexualized\\ substance\\ use\\ in\\ the\\ last\\ 6\\ months}$',\n",
    "                                                            'positive_test_chla': r'$\\mathbf{Chlamydia\\ diagnosis\\ in\\ the\\ past}$'})\n",
    "\n",
    "column_order = [r'$\\mathbf{Age}$ 18-29 years', '30-39 years (R)', '40-49 years', '50-80 years', r'$\\mathbf{HIV-specialists\\ density}$ 0-2', '3-5', '6-9 (R)', '10-13',\n",
    "                r'$\\mathbf{Monthly\\ net-equivalent\\ income}$ 1 - <1000€', '1000 - <2000€', '2000 - <3000€ (R)', '3000 - <4000€', '4000 - <5000€', r'$\\geq$ 5000€',\n",
    "                r'$\\mathbf{Satisfaction\\ with\\ sex\\ life}$ Discontent', 'Content (R)', \"Sex doesn't matter right now\", \n",
    "                r'$\\mathbf{Sex\\ partners\\ in\\ the\\ last\\ 6\\ months}$ 0', '1', '2-3', '4-5', '6-10', '11-20', r'$\\geq$ 20 (R)', \n",
    "                r'$\\mathbf{Condom\\ use}$ 0%', '25% (R)', '50%', '75%', '>95%',\n",
    "                r'$\\mathbf{Sexualized\\ substance\\ use\\ in\\ the\\ last\\ 6\\ months}$', \n",
    "                r'$\\mathbf{Chlamydia\\ diagnosis\\ in\\ the\\ past}$']\n",
    "\n",
    "# color by significantly associated with PrEP use (green) / non-PrEP use (orange), or not significantly associated (grey)\n",
    "column_colors = {r'$\\mathbf{Age}$ 18-29 years': 'orange', '30-39 years (R)': 'grey', '40-49 years': 'grey', '50-80 years': 'grey',\n",
    "                r'$\\mathbf{HIV-specialists\\ density}$ 0-2': 'grey', '3-5': 'grey', '6-9 (R)': 'grey', '10-13': 'seagreen',\n",
    "                r'$\\mathbf{Monthly\\ net-equivalent\\ income}$ 1 - <1000€': 'orange', '1000 - <2000€': 'grey', '2000 - <3000€ (R)': 'grey', '3000 - <4000€': 'grey', \n",
    "                '4000 - <5000€': 'grey', r'$\\geq$ 5000€': 'grey',\n",
    "                r'$\\mathbf{Satisfaction\\ with\\ sex\\ life}$ Discontent': 'orange', 'Content (R)': 'grey', \"Sex doesn't matter right now\": 'grey', \n",
    "                r'$\\mathbf{Sex\\ partners\\ in\\ the\\ last\\ 6\\ months}$ 0': 'grey', '1': 'orange', '2-3': 'orange', '4-5': 'orange', '6-10': 'grey', '11-20': 'seagreen', r'$\\geq$ 20 (R)': 'grey', \n",
    "                r'$\\mathbf{Condom\\ use}$ 0%': 'seagreen', '25% (R)': 'grey', '50%': 'orange', '75%': 'grey', '>95%': 'orange',\n",
    "                r'$\\mathbf{Sexualized\\ substance\\ use\\ in\\ the\\ last\\ 6\\ months}$': 'orange',\n",
    "                r'$\\mathbf{Chlamydia\\ diagnosis\\ in\\ the\\ past}$': 'seagreen'}\n",
    "\n",
    "color_palette = [column_colors.get(col, 'gray') for col in plot_bst_strata_renamed.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 10))\n",
    "sns.boxplot(data = plot_bst_strata_renamed , \n",
    "            orient = \"h\", \n",
    "            fliersize = 0.3,\n",
    "            linewidth = 0.8,\n",
    "            order = column_order,\n",
    "            palette= color_palette)\n",
    "plt.xlabel('Bootstrapped regression coefficients β')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.axvline(x = 0, color = 'grey', linestyle='--') # add vertical line at 0 to mark no effect \n",
    "no_prep = mpatches.Patch(color = 'orange', label = 'Non-PrEP use')\n",
    "prep = mpatches.Patch(color = 'seagreen', label = 'PrEP use')\n",
    "plt.legend(handles=[no_prep, prep])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value imputation \n",
    "check if missing value imputation is correct by encoding missing as a seperate category "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding\n",
    "same as in main analysis with the exception that for the categorical variables the missing values are treated as a seperate category and are encoded as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate PrEP users and non-PrEP users with a PrEP indication (n = 1,458)\n",
    "concatenated_population_sensitivity = pd.concat([PrEP_indication, no_PrEP_indication], ignore_index = True) \n",
    "\n",
    "# drop unnecessary columns (for multivariable analysis)\n",
    "concatenated_population_sensitivity.drop(['welle', 'status', 'reason_no_prep_low_HIV_risk',\n",
    "       'reason_no_prep_medical_reasons', 'reason_no_prep_no_doctor',\n",
    "       'reason_no_prep_talking_sexlife_doctor',\n",
    "       'reason_no_prep_effort_too_high', 'reason_no_prep_daily_pill',\n",
    "       'reason_no_prep_fear_reactions', 'reason_no_prep_fear_side_effects',\n",
    "       'HIV_SPECIALIST_DENSITY_grouped', 'gendersex'], axis = 1, inplace = True) \n",
    "\n",
    "print(f'Number of Variables (including the target variable): {len(concatenated_population_sensitivity.columns)}')\n",
    "\n",
    "\n",
    "# already binary variables --> transform into 0/1 \n",
    "concatenated_population_sensitivity.replace(to_replace = {'ja': 1, 'Ja': 1, 'nein': 0, 'Nein': 0}, inplace = True) \n",
    "\n",
    "# already binary w/o missing values --> no further encoding \n",
    "binary_columns = ['sex_with_men','sex_with_women','sex_with_nonbinary', \n",
    "                  'positive_test_syph', 'positive_test_go','positive_test_chla','no_positive_test',\n",
    "                  'positive_test_hcv', 'landbin']\n",
    "\n",
    "# categorical variables (including binary variables with NaNs) \n",
    "categorical_columns_sensitivity = ['ABSCHLUSS_TYPE', 'SEXLIFE', 'age', 'sex_partner', 'sex_freq', \n",
    "                                   'drugs_inject', 'condom',\n",
    "                                   'NET_EQUIVALENT_INCOME', 'URBAN_RURAL_AREA', 'HIV_SPECIALIST_DENSITY', \n",
    "                                   'test_syp_freq', 'test_go_freq', 'test_chla_freq', 'test_hcv_freq', 'sti_symptom',\n",
    "                                   'sexualized_drug_use', 'sexwork']\n",
    "\n",
    "# encode categorical variables as dummy variables (keep NaNs as a seperate category)\n",
    "encoded_population_sensitivity = pd.get_dummies(data = concatenated_population_sensitivity, \n",
    "                         columns = categorical_columns_sensitivity, \n",
    "                         dtype = int, dummy_na = True, \n",
    "                         drop_first = False)\n",
    "\n",
    "\n",
    "# dropping the reference category in the dummy encoded variables (most frequent)\n",
    "for col in categorical_columns_sensitivity:\n",
    "    most_frequent = concatenated_population_sensitivity[col].value_counts(dropna = False).idxmax() \n",
    "    most_frequent_dummy_col = f\"{col}_{most_frequent}\"\n",
    "    print(f'reference category for \"{col}\" is \"{most_frequent_dummy_col}\"')\n",
    "    encoded_population_sensitivity.drop(most_frequent_dummy_col, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariable analysis\n",
    "multivariable regression with new encoded data (includes NaN values as a seperate category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_nan = encoded_population_sensitivity.columns.drop('Target') \n",
    "feature_names_nan = encoded_population_sensitivity[feature_cols_nan].columns\n",
    "\n",
    "X_nan = encoded_population_sensitivity[feature_cols_nan].values \n",
    "y_nan = encoded_population_sensitivity['Target'] \n",
    "\n",
    "# load final selected model (same model as in main multivariable analysis)\n",
    "rng = np.random.RandomState(1)\n",
    "final_model = load('results/parameter_tuning_models/logreg_0_4_elbow.joblib') \n",
    "\n",
    "final_model.fit(X_nan, y_nan)\n",
    "sens_coef = final_model.coef_\n",
    "sens_coef_non_zero = sens_coef[np.nonzero(sens_coef)]\n",
    "sens_feature_names_non_zero = feature_names_nan[np.nonzero(sens_coef)[1]]\n",
    "sens_coef_non_zero_df = pd.DataFrame({'Feature': sens_feature_names_non_zero,\n",
    "                                   'Coefficient': sens_coef_non_zero})\n",
    "sens_coef_df = pd.DataFrame({'Feature': feature_names_nan,\n",
    "                             'Coefficient': sens_coef[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapped_coefficients_nan = []\n",
    "\n",
    "for _ in range(10000):\n",
    "    X_bst, y_bst = resample(X_nan, y_nan, \n",
    "                            random_state = rng, \n",
    "                            stratify = y_nan,\n",
    "                            replace = True)\n",
    "    final_model.fit(X_bst, y_bst)\n",
    "    bootstrapped_coefficients_nan.append(final_model.coef_[0])\n",
    "    \n",
    "bst_coef_nan  = pd.DataFrame(bootstrapped_coefficients_nan, columns = feature_names_nan)\n",
    "non_zero_bst_coef_nan = bst_coef_nan.loc[:, (bst_coef_nan  != 0).any(axis = 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentile confidence interval \n",
    "CI_lower_nan = np.percentile(np.array(bst_coef_nan), 2.5, axis = 0)\n",
    "CI_upper_nan = np.percentile(np.array(bst_coef_nan), 97.5, axis = 0)\n",
    "\n",
    "multivariable_coef_results_nan = sens_coef_df.copy()\n",
    "multivariable_coef_results_nan['lower 95% CI'] = CI_lower_nan\n",
    "multivariable_coef_results_nan['upper 95% CI'] = CI_upper_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_coefs_nan = (CI_lower_nan > 0) | (CI_upper_nan < 0) # significant coefficients based on confidence intervals (check: is 0 included in the CI)\n",
    "\n",
    "significant_from_CI_nan_df = pd.DataFrame({\n",
    "    'Feature': feature_names_nan,\n",
    "    'Lower 95% CI': CI_lower_nan,\n",
    "    'Upper 95% CI': CI_upper_nan,\n",
    "    'Significant': significant_coefs_nan\n",
    "})\n",
    "\n",
    "# compare this to significant based on bootstrapped p-value \n",
    "significant_from_CI_nan_df.loc[significant_from_CI_nan_df['Significant'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_samples = 10000\n",
    "p_values_features_nan = {}\n",
    "\n",
    "for feature in feature_names_nan:\n",
    "    bst_coefs_nan = bst_coef_nan[feature] # bootstrapped coefficients for the feature\n",
    "\n",
    "    greater_than_zero = np.sum(bst_coefs_nan >= 0) / bootstrap_samples # sum of bootstrapped coefficients >= zero / bootstrap samples\n",
    "    less_than_zero = np.sum(bst_coefs_nan <= 0) / bootstrap_samples # sum of bootstrapped coefficients <= zero / bootstrap samples\n",
    "    \n",
    "    p_value_nan = min(greater_than_zero, less_than_zero)\n",
    "    p_values_features_nan[feature] = p_value_nan\n",
    "\n",
    "\n",
    "p_values_features_nan_df = pd.DataFrame([p_values_features_nan]).T\n",
    "p_values_features_nan_df.rename(columns = {0: 'p_values'}, inplace = True)\n",
    "\n",
    "\n",
    "multivariable_coef_results_nan['p_values'] = p_values_features_nan_df['p_values'].values\n",
    "\n",
    "p_values_features_nan_df.sort_values(by = 'p_values', ascending = True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_2_decimal = ['Coefficient', 'lower 95% CI', 'upper 95% CI']\n",
    "columns_3_decimal = ['p_values']\n",
    "\n",
    "multivariable_coef_results_nan_rounded = multivariable_coef_results_nan.copy()\n",
    "multivariable_coef_results_nan_rounded[columns_2_decimal] = multivariable_coef_results_nan_rounded[columns_2_decimal].round(2)\n",
    "multivariable_coef_results_nan_rounded[columns_3_decimal] = multivariable_coef_results_nan_rounded[columns_3_decimal].round(3)\n",
    "\n",
    "multivariable_coef_results_nan_rounded.to_csv('results/multivariable_analysis_result_sensitivity.csv', index = True)\n",
    "\n",
    "multivariable_coef_results_nan_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14 significant features\n",
    "significant_features_multivariable_nan = multivariable_coef_results_nan.loc[multivariable_coef_results_nan['p_values'] < 0.05]['Feature']\n",
    "significant_bst_coef_nan = bst_coef_nan[significant_features_multivariable_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(significant_features_multivariable_nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one additional significant feature in sensitivity analysis: SEXLIFE_nan\n",
    "# rest of the significant features are the same as in the main multivariable analysis\n",
    "significant_features_multivariable_nan "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
